<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TreeSBA">
  <meta name="keywords" content="TreeSBA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TreeSBA</title>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <!-- Bootstrap -->
  <!-- <link rel="stylesheet" href="./static/css/bootstrap-4.4.1.css"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/72.png">

  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <script src="./static/js/app.js"></script>
  <script src="./static/js/video_comparison.js"></script>

  <link rel="stylesheet" href="./static/css/dics.original.css">
  <script src="./static/js/event_handler.js"></script>
  <script src="./static/js/dics.original.js"></script>
  
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="margin-bottom: 0"><strong>TreeSBA</strong></h1>
          <br>
          <h2 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">Tree-Transformer for Self-Supervised</h2>
          <h2 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">Sequential Brick Assembly</h2>
          <br>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a href="https://dreamguo.github.io/">Mengqi Guo*</a><sup></sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;

            <span class="author-block">
              <a href="https://chaneyddtt.github.io/">Chen Li*</a><sup></sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;

            <span class="author-block">
              <a href="https://yuyangzhao.com/">Yuyang Zhao</a><sup></sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;

            <span class="author-block">
              <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a><sup></sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>National University of Singapore</span><!-- &nbsp;&nbsp;&nbsp;&nbsp; -->
            
            <!-- <span class="author-block"><sup>2</sup>Princeton University</span> -->
            
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>* denotes equal contribution</span>            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://drive.google.com/file/d/13i3HeVBiqN8JXnwAzTvQrPz2rShxIhMv/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <!-- arXiv Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
          
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://niujinshuchong.github.io/mip-splatting-demo/"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <i class="fab fa-dribbble"></i>
                  </span>
                  <span>Viewer</span>
                  </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <img src="./resources/teaser.png" style="transform: scale(1.);" class="center">
          <h2 class="subtitle" style="margin-top: 25px; text-align: left;">
            Example of self-supervised sequential brick assembly. The 3D object is assembled from LEGO bricks based on the predicted sequence of actions.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: 00px">Sequential Brick Assembly Demo</h2>
        <video poster="" id="Assembly_Airplane" autoplay controls muted loop playsinline height="100%">
          <source src="./resources/Assembly_Airplane.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Inferring step-wise actions to assemble 3D objects with primitive bricks from images is a challenging task due to complex constraints and the vast number of possible combinations. Recent studies have demonstrated promising results on sequential LEGO brick assembly through the utilization of LEGO-Graph modeling to predict sequential actions. However, existing approaches are class-specific and require significant computational and 3D annotation resources.
            In this work, we first propose a computationally efficient breadth-first search (BFS) LEGO-Tree structure to model the sequential assembly actions by considering connections between consecutive layers. Based on the LEGO-Tree structure, we then design a class-agnostic tree-transformer framework to predict the sequential assembly actions from the input multi-view images. A major challenge of the sequential brick assembly task is that the step-wise action labels are costly and tedious to obtain in practice. We mitigate this problem by leveraging synthetic-to-real transfer learning. Specifically, our model is first pre-trained on synthetic data with full supervision from the available action labels. We then circumvent the requirement for action labels in the real data by proposing an action-to-silhouette projection that replaces action labels with input image silhouettes for self-supervision. Without any annotation on the real data, our model outperforms existing methods with 3D supervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction datasets, respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Reconstructions on the Tanks and Temples Dataset</h2>
    <div class="container">
      
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-caterpillar">
          <video poster="" id="caterpillar" autoplay controls muted loop playsinline height="100%">
            <source src="./resources/Caterpillar_ours_merge.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-barn">
          <video poster="" id="barn" autoplay controls muted loop playsinline height="100%">
            <source src="./resources/Barn_ours_merge.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-ignatius">
          <video poster="" id="ignatius" autoplay controls muted loop playsinline height="100%">
            <source src="./resources/Ignatius_ours_merge.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-truck">
          <video poster="" id="truck" autoplay controls muted loop playsinline height="100%">
            <source src="./resources/Truck_ours_merge.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Motivation</h2>
        <img src="./resources/LEGO_Tree.png" style="transform: scale(1.);" class="center">
        <div class="content has-text-justified">
          <p>
            Illustration of the LEGO-Tree model and action reordering data augmentation. Node number represents the index of action in the sequence and the index of brick in the tree. (a) shows a BFS LEGO-Tree (solid line) generated from a LEGO-Graph (dotted line). (b) is an action-reordering augmented sample of (a).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Framework. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Framework</h2>

        <img src="./resources/framework.png" style="transform: scale(1.);" class="center">
        <div class="content has-text-justified">
          <p>
            Overview of our two-stage framework that consists of a pre-training stage and a self-training stage. In the pre-training stage, we use an image encoder to extract the image features, which are fed into a brick decoder to estimate the action sequence. The estimated actions are supervised with ground truth action labels. In the self-training stage, we use voxelization and projection modules to transfer action probabilities into pixel probabilities such that the model can be self-supervised with the input images.
          </p>
        </div>
        
        <img src="./resources/projection.png" style="transform: scale(1.);" class="center">
        <div class="content has-text-justified">
          <p>
            An example to illustrate the silhouette self-training with the input images $I$. Given the action probabilities $P^A=[p_0,p_1,p_2]$, a 4-step approach is used to compute the pixel probabilities $P^I=[p^I_0,p^I_1,p^I_2]$.
          </p>
        </div>
      </div>
    </div>

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: 30px">Results</h2>

        <img src="./resources/result_main.png" style="transform: scale(1.);" class="center">
        <div class="content has-text-justified">
          <p>
            Visualization of the assembled objects in the ModelNet40-C. `Ours-SP' is the result of the model only with pre-training.
          </p>
        </div>
        
        <img src="./resources/result_ablation.png" style="transform: scale(1.);" class="center">
        <div class="content has-text-justified">
          <p>
            Visual comparison of baselines, brick number, brick size.
          </p>
        </div>
      </div>
    </div>

  </div>

</section>

<!-- BibTeX. -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{Guo2024TreeSBA,
  author    = {Guo, Mengqi and Li, Chen and Zhao, Yuyang and Lee, Gim Hee},
  title     = {TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly},
  journal   = {European Conference on Computer Vision (ECCV)},
  year      = {2024},
}</code></pre>
  </div>
</section>

<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    We thank Christian Reiser for insightful discussions and valuable feedback throughout the project.
    We also thank Binbin Huang for proofreading. 
    ZY and AG are supported by the ERC Starting Grant LEGO-3D (850533) and DFG EXC number 2064/1 - project number 390727645.
    TS is supported by a Czech Science Foundation (GACR) EXPRO grant (UNI-3D, grant no. 23-07973X).
  </div>
</section> -->

<section class="section" id="References">
  <div class="container is-max-desktop content">

        <h3 class="title is-4">References</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <a href="https://github.com/POSTECH-CVLab/Brick-by-Brick" target="_blank">
                Brick-by-Brick: Combinatorial Construction with Deep Reinforcement Learning</a>
            </li>

            <li>
              <a href="https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation" target="_blank">
                Combinatorial 3D Shape Generation via Sequential Assembly</a>
            </li>

          </ul>
        </div>
      </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template. 
            The video comparison with sliding bar is from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>. 
            The image comparison with sliding bar is from <a href="https://research.nvidia.com/labs/dir/neuralangelo/">Neuralangelo</a>. 
          </p>
        </div>
      </div>
        
    </div>
  </div>
</footer>

</body>
</html>
